{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolution and Relu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a convolutional base and a head of dense layers. We learned that the job of the base is to extract visual features from an image, which the head would then use to classify the image. These are the convolutional layer with ReLU activation, and the maximum pooling layer\n",
        "\n",
        "The feature extraction performed by the base consists of three basic operations:\n",
        "    Filter an image for a particular feature (convolution)\n",
        "    Detect that feature within the filtered image (ReLU)\n",
        "    Condense the image to enhance the features (maximum pooling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-2-0a2a22fb343d>, line 11)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-0a2a22fb343d>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    layers.Conv2D(filters=64, kernel_size=3)\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Filter de Convolution - A convolutional layer carries out the filtering step\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    \"\"\" \n",
        "    - filters: Integer, the dimensionality of the output space (the number of output filters in the convolution).\n",
        "    - kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. \n",
        "        Can be a single integer to specify the same value for all spatial dimensions \n",
        "    \"\"\"\n",
        "    layers.Conv2D(filters=64, kernel_size=3)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can understand these parameters by looking at their relationship to the weights and activations of the layer.\n",
        "    - The weights a convnet learns during training are primarily contained in its convolutional layers. These weights we call kernels. We can represent them as small arrays\n",
        "\n",
        "    A kernel operates by scanning over an image and producing a weighted sum of pixel values. In this way, a kernel will act sort of like a polarized lens, emphasizing or deemphasizing certain patterns of information\n",
        "\n",
        "    Kernels define how a convolutional layer is connected to the layer that follows. The kernel above will connect each neuron in the output to nine neurons in the input. By setting the dimensions of the kernels with kernel_size, you are telling the convnet how to form these connections\n",
        "\n",
        "    The kernels in a convolutional layer determine what kinds of features it creates. During training, a convnet tries to learn what features it needs to solve the classification problem. This means finding the best values for its kernels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Activations - The activations in the network we call feature maps. They are what result when we apply a filter to an image; they contain the visual features the kernel extracts. From the pattern of numbers in the kernel, you can tell the kinds of feature maps it creates. With the filters parameter, you tell the convolutional layer how many feature maps you want it to create as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ReLU - After filtering, the feature maps pass through the activation function, the rectifier function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ReLU\n",
        "model2 = keras.Sequential([\n",
        "    layers.Conv2D(filters=64, kernel_size=3, activation='relu')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You could think about the activation function as scoring pixel values according to some measure of importance. The ReLU activation says that negative values are not important and so sets them to 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Condense with Maximum Pooling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model3 = keras.Sequential([\n",
        "    layers.Conv2D(filters=64, kernel_size=3),\n",
        "    layers.MaxPool2D(pool_size=2)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A MaxPool2D layer is much like a Conv2D layer, except that it uses a simple maximum function instead of a kernel, with the pool_size parameter analogous to kernel_size. A MaxPool2D layer doesn't have any trainable weights like a convolutional layer does in its kernel, however.\n",
        "Notice that after applying the ReLU function (Detect) the feature map ends up with a lot of \"dead space,\" that is, large areas containing only 0's (the black areas in the image). Having to carry these 0 activations through the entire network would increase the size of the model without adding much useful information. Instead, we would like to condense the feature map to retain only the most useful part -- the feature itself.\n",
        "This in fact is what maximum pooling does. Max pooling takes a patch of activations in the original feature map and replaces them with the maximum activation in that patch. When applied after the ReLU activation, it has the effect of \"intensifying\" features. The pooling step increases the proportion of active pixels to zero pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" We'll use another one of the functions in tf.nn to apply the pooling step, tf.nn.pool. This is a Python function that does the same thing as the MaxPool2D layer \n",
        "    you use when model building, but, being a simple function, is easier to use directly. \"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "image_condense = tf.nn.pool(\n",
        "    input=image_detect, # image in the Detect step above\n",
        "    window_shape=(2, 2),\n",
        "    pooling_type='MAX',\n",
        "    # we'll see what these do in the next lesson!\n",
        "    strides=(2, 2),\n",
        "    padding='SAME',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pooling, Maximum Pooling and Average\n",
        "Often, as we process images, we want to gradually reduce the spatial resolution of our hidden representations, aggregating information so that the higher up we go in the network, the larger the receptive field (in the input) to which each hidden node is sensitive. Like convolutional layers, pooling operators consist of a fixed-shape window that is slid over all regions in the input according to its stride, computing a single output for each location traversed by the fixed-shape window (sometimes known as the pooling window). However, unlike the cross-correlation computation of the inputs and kernels in the convolutional layer, the pooling layer contains no parameters (there is no kernel). nstead, pooling operators are deterministic, typically calculating either the maximum or the average value of the elements in the pooling window. These operations are called maximum pooling (max pooling for short) and average pooling, respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" In the code below, we implement the forward propagation of the pooling layer in the pool2d function. here we have no kernel, computing the output as either \n",
        "the maximum or the average of each region in the input. \"\"\"\n",
        "\n",
        "def pool2d(X, pool_size, mode='max'):\n",
        "    p_h, p_w = pool_size\n",
        "    Y = tf.Variable(tf.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + '')))\n",
        "    for i in range(Y.shape[1]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            if mode == 'max':\n",
        "                Y[i, j].assign(tf.reduce_max(X[i: i + p_h, j: j +p_w]))\n",
        "            elif mode == 'avg':\n",
        "                Y[i, j].assign(tf.reduce_mean(X[i: i + p_h, j: j + p_w]))\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Translation Invariance\n",
        "We called the zero-pixels \"unimportant\". Does this mean they carry no information at all? In fact, the zero-pixels carry positional information. The blank space still positions the feature within the image. When MaxPool2D removes some of these pixels, it removes some of the positional information in the feature map. This gives a convnet a property called translation invariance. This means that a convnet with maximum pooling will tend not to distinguish features by their location in the image. (\"Translation\" is the mathematical word for changing the position of something without rotating it or changing its shape or size.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Sliding Window\n",
        "The convolution and pooling operations share a common feature: they are both performed over a sliding window. With convolution, this \"window\" is given by the dimensions of the kernel, the parameter kernel_size. With pooling, it is the pooling window, given by pool_size. There are two additional parameters affecting both convolution and pooling layers -- these are the strides of the window and whether to use padding at the image edges. The strides parameter says how far the window should move at each step, and the padding parameter describes how we handle the pixels at the edges of the inpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'keras' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-2-141fd1a2a22a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model4 = keras.Sequential([\n\u001b[0m\u001b[0;32m      2\u001b[0m     layers.Conv2D(\n\u001b[0;32m      3\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ],
      "source": [
        "model4 = keras.Sequential([\n",
        "    layers.Conv2D(\n",
        "        filter=64,\n",
        "        kernel_size=3,\n",
        "        strides=1,\n",
        "        padding='same',\n",
        "        activation='relu'\n",
        "    ),\n",
        "    layers.MaxPool2D(\n",
        "        pool_size=2,\n",
        "        strides=1,\n",
        "        padding='same'\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Stride - The distance the window moves at each step is called the stride. We need to specify the stride in both dimensions of the image: one for moving left to right and one for moving top to bottom.  convolutional layers will most often have strides=(1, 1). Increasing the stride means that we miss out on potentially valuble information in our summary. Maximum pooling layers, however, will almost always have stride values greater than 1, like (2, 2) or (3, 3), but not larger than the window itself.\n",
        "\n",
        "- Padding - As described above, one tricky issue when applying convolutional layers is that we tend to lose pixels on the perimeter of our image. Since we typically use small kernels, for any given convolution, we might only lose a few pixels, but this can add up as we apply many successive convolutional layers. One straightforward solution to this problem is to add extra pixels of filler around the boundary of our input image, thus increasing the effective size of the image\n",
        "\n",
        "CNNs commonly use convolution kernels with odd height and width values, such as 1, 3, 5, or 7. Choosing odd kernel sizes has the benefit that we can preserve the spatial dimensionality while padding with the same number of rows on top and bottom, and the same number of columns on left and right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolutions for images\n",
        "in such a layer, an input tensor and a kernel tensor are combined to produce an output tensor through a cross-correlation operation. When the convolution window slides to a certain position, the input subtensor contained in that window and the kernel tensor are multiplied elementwise and the resulting tensor is summed up yielding a single scalar value. This result gives the value of the output tensor at the corresponding location. the output size is slightly smaller than the input size. Because the kernel has width and height greater than one, we can only properly compute the cross-correlation for locations where the kernel fits wholly within the image, the output size is given by the input size  minus the size of the convolution kernel (Nh - Kh + 1) x (Nw + Kw + 1) - n (Input shape), k (kernel shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from d2l import tensorflow as d2l\n",
        "\n",
        "def corr2d(X, K):\n",
        "    \"\"\" Compute 2D cross-correlation \"\"\"\n",
        "    h, w = K.shape # h (height), w (width)\n",
        "    Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[0]):\n",
        "            Y[i, j].assign(tf.reduce_sum(\n",
        "                X[i: i + h, j: j + w] * K\n",
        "            ))\n",
        "\n",
        "\"\"\" We can construct the input tensor X and the kernel tensor K to validate the output of the above implementation of the two-dimensional cross-correlation operation. \"\"\"\n",
        "X = tf.constant([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]]) # Input shape\n",
        "K = tf.constant([[0.0, 1.0], [2.0, 3.0]]) # Kernel size\n",
        "corr2d(X, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolutional layer\n",
        "A convolutional layer cross-correlates the input and kernel and adds a scalar bias to produce an output. The two parameters of a convolutional layer are the kernel and the scalar bias. When training models based on convolutional layers, we typically initialize the kernels randomly, just as we would with a fully-connected layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Conv2D(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, kernel_size):\n",
        "        initializer = tf.random_normal_initializer()\n",
        "        self.weight = self.add_weight(\n",
        "            name=\"w\", \n",
        "            shape=kernel_size,\n",
        "            initializer=initializer\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return corr2d(inputs, self.weight) + self.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Learning a Kernel\n",
        "We first construct a convolutional layer and initialize its kernel as a random tensor. Next, in each iteration, we will use the squared error to compare Y with the output of the convolutional layer. We can then calculate the gradient to update the kernel. For the sake of simplicity, in the following we use the built-in class for two-dimensional convolutional layers and ignore the bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-4f48beae5586>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Construct a two-dimensional convolutional layer with 1 output channel and a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mconv2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# The two-dimensional convolutional layer uses four-dimensional input and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "# Construct a two-dimensional convolutional layer with 1 output channel and a\n",
        "# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\n",
        "conv2d = tf.layers.Conv2D(1, (1, 2), use_bias=False)\n",
        "\n",
        "# The two-dimensional convolutional layer uses four-dimensional input and\n",
        "# output in the format of (example, height, width, channel), where the batch\n",
        "# size (number of examples in the batch) and the number of channels are both 1\n",
        "X = tf.reshape(X, (1, 6, 8, 1))\n",
        "Y = tf.reshape(Y, (1, 6, 7, 1))\n",
        "lr = 3e-2 # Learning rate\n",
        "\n",
        "Y_hat = conv2d(X)\n",
        "for i in range(10):\n",
        "    with tf.GradientTape(watch_accessed_variables=False) as g:\n",
        "        g.watch(conv2d.weights[0])\n",
        "        Y_hat = conv2d(X)\n",
        "        l = (abs(Y_hat - Y)) ** 2\n",
        "        \n",
        "        #Update de kernel\n",
        "        update = tf.multiply(lr, g.gradient(1, conv2d.weights[0]))\n",
        "        weights = conv2d.get_weighs()\n",
        "        weights[0] = conv2d.weights[0] - update\n",
        "        conv2d.set_weights(weights)\n",
        "        if (i + 1) % 2 == 0:\n",
        "            print(f'epoch {i + 1}, loss {tf.reduce_sum(1):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.reshape(conv2d.get_weights()[0], (1, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Map and Receptive Field\n",
        "As described in Section, the convolutional layer output is sometimes called a feature map, as it can be regarded as the learned representations (features) in the spatial dimensions (e.g., width and height) to the subsequent layer. In CNNs, for any element  of some layer, its receptive field refers to all the elements (from all the previous layers) that may affect the calculation of  during the forward propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pading\n",
        "In the following example, we create a two-dimensional convolutional layer with a height and width of 3 and apply 1 pixel of padding on all sides. Given an input with a height and width of 8, we find that the height and width of the output is also 8. As with convolutional layers, pooling layers can also change the output shape. And as before, we can alter the operation to achieve a desired output shape by padding the input and adjusting the stride. We can demonstrate the use of padding and strides in pooling layers via the built-in two-dimensional maximum pooling layer from the deep learning framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We define a convenience function to calculate the convolutional layer. This\n",
        "# function initializes the convolutional layer weights and performs\n",
        "# corresponding dimensionality elevations and reductions on the input and\n",
        "# outpu\n",
        "def comp_conv2d(conv2d, X):\n",
        "    \"\"\" Here (1, 1) indicates that the batch size and the number of channels are both 1 \"\"\"\n",
        "    X = tf.reshape(X, (1, ) + X.shape + (1, ))\n",
        "    Y = conv2d(X)\n",
        "    # Exclude the first two dimensions that do not interest us: examples and channels\n",
        "    return tf.reshape(Y, Y.shape[1:3])\n",
        "\n",
        "\"\"\" Note that here 1 row or column is padded on either side, so a total of 2 two rows or columns are added \"\"\"\n",
        "conv2d = tf.keras.layers.Conv2D(1, kernel_size=3, padding='same')\n",
        "X = tf.random.uniform(shape=(8, 8))\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" When the height and width of the convolution kernel are different, we can make the output and input have the same height and width by setting different \n",
        "padding numbers for height and width. \"\"\"\n",
        "\n",
        "# Here, we use a convolution kernel with a height of 5 and a width of 3. The\n",
        "# padding numbers on either side of the height and width are 2 and 1,\n",
        "# respectively\n",
        "conv2d = tf.keras.layers.Conv2D(1, kernel_size=(5, 3), padding='same')\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stride\n",
        "When computing the cross-correlation, we start with the convolution window at the upper-left corner of the input tensor, and then slide it over all locations both down and to the right. Below, we set the strides on both the height and width to 2, thus halving the input height and width."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv2d = tf.keras.layers.Conv2D(1, kernel_size=3, padding='same', strides=2)\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multiple Input Channels\n",
        "When the input data contain multiple channels, we need to construct a convolution kernel with the same number of input channels as the input data, so that it can perform cross-correlation with the input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we can implement cross-correlation operations with multiple input channels ourselves\n",
        "def corr2d_multi_in(X, K):\n",
        "    # First, iterate through the 0th dimesion (channel dimension) of 'X' and 'Y'\n",
        "    return tf.reduce_sum([d2l.corr2d(x, k) for x, k in zip(X, K)], axis=0)\n",
        "\n",
        "# We can construct the input tensor X and the kernel tensor K corresponding to the values to validate the output of the cross-correlation operation.\n",
        "X = tf.constant([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
        "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
        "K = tf.constant([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
        "\n",
        "corr2d_multi_in(X, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multipe Output Channels\n",
        "In cross-correlation operations, the result on each output channel is calculated from the convolution kernel corresponding to that output channel and takes input from all channels in the input tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#We implement a cross-correlation function to calculate the output of multiple channels\n",
        "def corr2d_multi_in_out(X, K):\n",
        "    # Iterate through the 0th dimension of `K`, and each time, perform\n",
        "    # cross-correlation operations with input `X`. All of the results are\n",
        "    # stacked together\n",
        "    return tf.stack([corr2d_multi_in(X, K), for k in K], 0)\n",
        "\n",
        "# We construct a convolution kernel with 3 output channels by concatenating the kernel tensor K with K+1 (plus one for each element in K) and K+2.\n",
        "K = tf.stack((K, K + 1, K + 2), 0)\n",
        "K.shape\n",
        "\n",
        "# Below, we perform cross-correlation operations on the input tensor X with the kernel tensor K. Now the output contains 3 channels. \n",
        "# The result of the first channel is consistent with the result of the previous input tensor X and the multi-input channel, single-output channel kernel.\n",
        "corr2d_multi_in_out(X, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary\n",
        "- Taking the input elements in the pooling window, the maximum pooling operation assigns the maximum value as the output and the average pooling operation assigns the average value as the output.\n",
        "\n",
        "- One of the major benefits of a pooling layer is to alleviate the excessive sensitivity of the convolutional layer to location.\n",
        "\n",
        "- We can specify the padding and stride for the pooling layer.\n",
        "\n",
        "- Maximum pooling, combined with a stride larger than 1 can be used to reduce the spatial dimensions (e.g., width and height).\n",
        "\n",
        "- The pooling layerâ€™s number of output channels is the same as the number of input channels"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "natural_language_processing.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "b1c48bfa55aad10950a784971f566084dc4f1d1e56735e6727b7d14ae8182804"
    },
    "kernelspec": {
      "display_name": "Python 3.7.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
